\subsection{Statistics versus DM/ML}

\textbf{Statistics} have been around for a while. Famous statisticians are for example:
\begin{itemize}
  \item John Graunt (1620-1674), who studied London's death records around 1660.
  \begin{itemize}
    \item He was able to predict the life expectance of a person at a particular age and was the first to create a "life table" with the probability of death for each age.
  \end{itemize}
  \item Francis Galton (1822-1911), who introduced many core statistical concepts at the end of the 19th century.
  \begin{itemize}
    \item He (re)invented variance, normal distribution, correlation, linear regression, etc.
  \end{itemize}
\end{itemize}

Back then, statistics were concerned with the problem of making generalizations based of relatively little data. Since then, the availability of data changed drastically, with now having more of an overload of data. Therefore, more \textbf{pragmatic} instead of statistical approaches for handling large amounts of data where introduced to fuel the progress in data science.
\begin{itemize}
  \item Major breakthroughs in the discovery of patterns and relationships are for example efficiently learning decision trees and association rules.
  \item By traditional statisticians, these were described as "data fishing", "data snooping", or "data degrading" \textcolor{gray}{\footnotesize(Surprisingly, some statisticians claim "owning" the data science field)}
\end{itemize}

Modern statisticians are now also concerned with a more pragmatic approach. Leo Breiman (1928-2005) wrote a paper ("Statistical Modeling: The Two Cultures") about the two main camps of statisticians:
\begin{itemize}
  \item The "classical statistics camp" ($98\%$) assumes nature's behavior to fit some model and focuses on parameter estimation and goodness-of-fit tests.
  \begin{itemize}
    \item An important aspect of this approach is \textbf{hypothesis testing}, which has led to the image of statisticians aiming to prove that nothing can be concluded from basically any given data while still data can be "tortured until confession", creating wrong conclusions.
  \end{itemize}
  \item The other $2\%$ of statisticians focus on simply finding a predictive function evaluated by predictive accuracy only (which fits the pragmatic approach).
  \item John W. Tukey (1915-2000), whose one of those $2\%$ focussed on practical statistics.
  \begin{itemize}
    \item This includes \textbf{exploratory data analysis} instead of hypothesis testing.
    \item For example, he invented boxplots.
  \end{itemize}
\end{itemize}

So to summarize the concept shift and also the difference between classical statistics and machine learning approaches:
\begin{itemize}
  \item Before, a small amount of data or only samples of the whole data distribution were available, whereas
  \item Now, we have a big amount of data or all available data (due to computing power, storage, and tools).
  \begin{itemize}
    \item The new approach is, therefore, to "let the data speak", since it's there.
  \end{itemize}
\end{itemize}

Problems, even with this new approach, are:
\begin{itemize}
  \item Data is always dirty, biased, etc. Fortunately, summarizing it can be surprisingly useful.
  \item Typical risks, raising the necessity of handling the new approach with care, are:
  \begin{itemize}
    \item Testing of many hypotheses,
    \item Over- or underfitting the data, and
    \item Having a bias in the data or the representation.
  \end{itemize}
\end{itemize}

