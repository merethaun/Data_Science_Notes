\subsection{Characterizing individual features}

As a first step of actual data exploration, we are now going to look at which information we can get from a single data feature. In terms of tabular data: we're going to focus on a single column.

What kind of data we can derive from the feature, depends of course on the data type. Generally deriving features from other ones is of course done best when dealing with structured data. Here, we have two types from which we can derive different properties.

From \textbf{continuous features}, we can derive\sidenote{Investigation of individual \textbf{continuous} features}\renewcommand{\arraystretch}{1.5}

\begin{tabular}{@{}>{\raggedleft}m{0.2\textwidth} @{}>{\color{black}\centering:}m{0.025\textwidth} @{}>{\color{black}}m{0.775\textwidth}}
  count && Number of instances having this feature \\
  \% miss && Percentage of missing information {\color{gray}\footnotesize(how many instances don't have this feature)} \\
  card && Number of unique values (cardinality) \\
  min && Minimal value over all instances \\
  1\textsuperscript{st} qrt && 25\textsuperscript{th} percentile {\color{gray}\footnotesize(largest value of the quarter of instances having the lowest values)} \\
  mean && Average value over all instances \\
  median && Middle value of all instances \\
  3\textsuperscript{rd} qrt && 75\textsuperscript{th} percentile {\color{gray}\footnotesize(smallest value of the quarter of instances having the highest values)} \\
  max && Maximal value over all instances \\
  std. dev && Standard deviation over all instances
\end{tabular}


From \textbf{categorical features}, we can derive\sidenote{Investigation of individual \textbf{categorical} feature}\footnote{obvious: $\min$, $\max$, {\color{mathblue}mean}, etc. can't be computed}

\begin{tabular}{@{}>{\raggedleft}m{0.2\textwidth} @{}>{\color{black}\centering:}m{0.025\textwidth} @{}>{\color{black}}m{0.775\textwidth}}
  count && Number of instances having this feature \\
  \% miss && Percentage of missing information {\color{gray}\footnotesize(how many instances don't have this feature)} \\
  card && Number of unique values (cardinality) \\
  mode && Most common value \\
  mode frequ && Frequency of the mode \\
  mode \% && Percentage of the mode \\
  2\textsuperscript{nd}\,mode && Second most common value \\
  2\textsuperscript{nd}\,mode frequ && Frequency for the second mode \\
  2\textsuperscript{nd}\,mode \% && Percentage of the second mode
\end{tabular}

To get a better idea of how to get these properties for all of the features, we will look at an example. Consider a table containing information about insurance claims fraud. The dataset contains 500 instances (claims) and a bunch of different features such as type, claim amount, etc. Now first, determine the data type of each feature, and then create one table for the numerical and one for the categorical features and fill it with the according information. The raw data can be found in \ref{fig:2_single_feature_example}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{assets/visualization_and_extraction/single_feature_example/raw_data.png}
  \caption{Example for single feature investigation (insurance claim fraud)}
  \label{fig:2_single_feature_example}
\end{figure}

To investigate the raw data further, let's first extract the resulting feature-describing tables and then also visualize the data. More specifically for the visualization, we're going to show the distributions of the different features. Figure \ref{fig:2_distr_visualization} shows both the properties of the features and examplary plots.
\begin{itemize}
  \item For finite amounts of possible feature classes, simply visualize the distribution as a bar diagram with the different classes as entries on the x-axis. The y-axis can either be the frequency or a percentage.
  \item For continuous features with continuous variables/infinitely many possible feature values, group items (\textbf{binning})\sidenote{Binning} and then visualize the resulting histogram.
\end{itemize}

\begin{figure}[H]
  \centering
  \subcaptionbox{Categorical features}{
    \includegraphics[width=\textwidth]{assets/visualization_and_extraction/single_feature_example/categorical.png}
  }
  \\\vspace*{0.25cm}
  \subcaptionbox{Numerical (continuous) features}{
    \includegraphics[width=\textwidth]{assets/visualization_and_extraction/single_feature_example/continuous.png}
  }
  \caption{Feature-discribing table and distribution visualization}
  \label{fig:2_distr_visualization}
\end{figure}

The binning comes with some challenges. When we select the amount of bins with evenly distributed width of each individual bin, we need to be aware not to \textbf{over- or underfit}\sidenote{Over- and underfitting for binning}. Examples can be found in \ref{fig:2_binning}. As one can see:
\begin{itemize}
  \item In the case of underfitting, the true function is not at all matched.
  \item In the case of overfitting, there exist very steep valeys, the provided data points are more learned by heart rather than abstracting to a function.
\end{itemize}

\begin{figure}[H]
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{assets/visualization_and_extraction/single_feature_example/bin_good.png}
    \subcaption{Good approximation\\$\qquad \qquad14 \text{ \color{mathblue} bins}$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{assets/visualization_and_extraction/single_feature_example/bin_underfitting.png}
    \subcaption{Underfitting\\$3 \text{ \color{mathblue} bins}$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{assets/visualization_and_extraction/single_feature_example/bin_overfitting.png}
    \subcaption{Overfitting\\$60 \text{ \color{mathblue} bins}$}
  \end{subfigure}
  \caption{Binning for continuous variables}
  \label{fig:2_binning}
\end{figure}

The histograms furthermore can show different types, as depicted in \ref{fig:2_histogram_types}.

\begin{figure}[H]
  \centering

  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{assets/visualization_and_extraction/single_feature_example/distr_uniform.png}
    \subcaption{Uniform}
  \end{subfigure}\hspace*{0.01\textwidth}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{assets/visualization_and_extraction/single_feature_example/distr_uni_normal.png}
    \subcaption{Unimodal, normal}
  \end{subfigure}\hspace*{0.01\textwidth}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{assets/visualization_and_extraction/single_feature_example/distr_uni_right.png}
    \subcaption{Unimodal, skewed right}
  \end{subfigure}
  
  \vspace*{0.1cm}
  
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{assets/visualization_and_extraction/single_feature_example/distr_uni_left.png}
    \subcaption{Unimodal, skewed left}
  \end{subfigure}\hspace*{0.01\textwidth}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{assets/visualization_and_extraction/single_feature_example/distr_exp.png}
    \subcaption{Exponential}
  \end{subfigure}\hspace*{0.01\textwidth}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{assets/visualization_and_extraction/single_feature_example/distr_multi.png}
    \subcaption{Multimodal}
  \end{subfigure}

  \caption{Histogram types}
  \label{fig:2_histogram_types}
\end{figure}

Here are some further notes on the types:
\begin{itemize}
  \item \textbf{Uniform}\sidenote{Uniform} means all items have the same likelyhood (within a range).
  \item \textbf{Unimodal}\sidenote{Unimodal} means we have one peak (can be tilted to one side), whereas \textbf{multimodal}\sidenote{Multimodal} means there are multiple distinct ones.
  \item \textbf{Exponential}\sidenote{Exponential} means we have an exponential descrease in the likelihood over all instances.
\end{itemize}


\subsubsection*{Normal distribution}

One of the types mentioned, we're now gonna investigate a bit further. The \textbf{normal distribution}\sidenote{Normal distribution} is described by two important variables, whose effects on the distribution are shown in \ref{fig:2_normal}:
\begin{itemize}
  \item The \textbf{mean}\sidenote{Mean}$\mu$, so the expected value also characterizing the peak, and
  \item The \textbf{standard deviation}\sidenote{Standard deviation}$\sigma$ characterizing how narrow the peak, or the distribution around the peak, is.
\end{itemize}

\begin{figure}[H]
  \centering
  \subcaptionbox{Variation of mean $\mu$}{
    \includegraphics[width=0.4\textwidth]{assets/visualization_and_extraction/norm_mean.png}
  }
  \hspace*{0.05\textwidth}
  \subcaptionbox{Variation of standard deviation $\sigma$}{
    \includegraphics[width=0.4\textwidth]{assets/visualization_and_extraction/norm_var.png}
  }
  \caption{Normal distribution}
  \label{fig:2_normal}
\end{figure}

The normal probability distribution over $x$ is defined as:
\begin{align*}
  x \sim&\, \mathcal{N}(\mu, \sigma) \\
  p(x) = &\,\frac{1}{\sqrt{2\pi \sigma^2}} \cdot \exp\left[ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2 \right]
\end{align*}

Interesting are now precise areas we instantly know something about. The $68$-$95$-$99.7$-rule\sidenote{$68$-$95$-$99.7$-rule} tells us, as depicted in \ref{fig:2_three_sigma}.
\begin{itemize}
  \item $68\%$ of all observations will be within $1\sigma$-distance of the mean,
  \item $95\%$ of all observations will be within $2\sigma$-distance of the mean, and
  \item $99.7\%$ of all observations will be within $3\sigma$-distance of the mean
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{assets/visualization_and_extraction/norm_rule.png}
  \caption{$68$-$95$-$99.7$-rule}
  \label{fig:2_three_sigma}
\end{figure}

From this rule, we can now derive probabilities for different events. Consider the following examples, also visualized in \ref{fig:2_three_sigma_examples}:
\begin{itemize}
  \item Example 1 is interested in the amount of defects for some produced item. The \textbf{tolerance} can be defined as within the $2\sigma$-range, so with:
  \begin{itemize}
    \item \textbf{LSL} (lower spcification limit)\sidenote{LSL} at $\mu-2\sigma$, meaning only $2.5\%$ of the instances have a larger deviation into the negative direction from the mean than this limit, and
    \item \textbf{USL} (upper spcification limit)\sidenote{USL} at $\mu-2\sigma$, meaning only $2.5\%$ of the instances have a larger deviation into the positive direction from the mean than this limit.
  \end{itemize}
  Combined, $100\%-2.5\%-2.5\%=95\%$ have a deviation from the mean lying withing the defined range.
  \item Example 2 is interested in how many deliveries are too late. Therefore, it \textbf{only} defined an \textbf{upper bound} with the USL at $\mu+2\sigma$. This means, $100\%-2.5\%=97.5\%$ of the deliveries are not to late.
\end{itemize}

\begin{figure}[H]
  \centering
  \subcaptionbox{Example 1 (defect tolerance)}{
    \includegraphics[width=0.4\textwidth]{assets/visualization_and_extraction/norm_rule_example_1.png}
  }
  \hspace*{0.05\textwidth}
  \subcaptionbox{Example 2 (delivery times)}{
    \includegraphics[width=0.4\textwidth]{assets/visualization_and_extraction/norm_rule_example_2.png}
  }
  \caption{Examples for $68$-$95$-$99.7$-rule}
  \label{fig:2_three_sigma_examples}
\end{figure}

Furthermore, the rule also introduces the term of \textbf{six sigma} or also lean six sigma. It basically means: Processes operatiing with "six sigma quality" are assumed to have $< 3.4$ defects per million instances, so $\Pr(\text{error})=0.0000034$. It characterizes a process improvement approach. This likelyhood is a combination of $\pm6\sigma$ tolerance and a "drift" of $\pm1.5\sigma$. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{assets/visualization_and_extraction/lean_six_sigma.png}
  \caption{Lean six sigma}
  \label{fig:2_lean_six_sigma}
\end{figure}
